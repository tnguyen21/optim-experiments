# Muon Optimal Configuration for ViT-Medium
# Learning rate scaled for smaller batch size due to memory constraints
# ~25M parameters
experiment_name: "vit_medium_cifar10_muon_optimal"
seed: 42
device: "cuda"  # or "cpu"

# Data settings
dataset: "cifar10"
data_dir: "./data"
num_workers: 8
train_val_split: 0.9  # 90% train, 10% val from training set

# Model settings
model: "vit_medium"
model_scale: "medium"  # ~25M parameters
num_classes: 10

# Training settings
batch_size: 1024  # Further reduced for memory efficiency with larger model
num_epochs: 3
eval_every: 10  # Evaluate every N epochs

# Muon Optimizer settings (LR scaled down for smaller batch size)
optimizer:
  type: "muon"
  lr: 0.0025  # 0.01 * (1024/4096) = 0.0025 (linear scaling rule)
  momentum: 0.95
  weight_decay: 0.1  # Muon typically uses higher weight decay
  nesterov: true
  adjust_lr_fn: "original"

# Logging
log_dir: "./experiments/logs"